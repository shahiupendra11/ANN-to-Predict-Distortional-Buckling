{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       H     B      D       t        Kd\n",
      "0  1.625  1.25  0.188  0.0188  4.858459\n",
      "1  1.625  1.25  0.188  0.0283  3.496159\n",
      "2  1.625  1.25  0.188  0.0312  3.165983\n",
      "3  1.625  1.25  0.188  0.0346  2.966884\n",
      "4  2.500  1.25  0.188  0.0188  4.360603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load Dataset (Assuming 'df' is already loaded)\n",
    "df = pd.read_excel('test.xlsx')\n",
    "# 'Kd' is the target, and all other columns are features\n",
    "X = df[['H', 'B', 'D', 't']].values  # 4 input features\n",
    "y = df['Kd'].values.reshape(-1, 1)  # Ensure correct shape\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights:\n",
      " [-2.15366560e-02 -7.52658839e-02  2.15878364e-01  1.42712132e+01\n",
      "  2.40604438e-01 -1.96066784e+00 -1.06366673e+00  1.16826808e-01\n",
      " -8.94233299e+00  2.13933889e+01  7.13756080e+03  4.17283261e+03\n",
      " -4.17385707e+03]\n",
      "Training MSE: 0.0424, MAE: 0.1428, R²: 0.9701\n",
      "Test MSE: 0.0560, MAE: 0.1777, R²: 0.9627\n",
      "    Actual (y_test)  Predicted (y_pred)  Error (%)\n",
      "0          3.437131            3.193715   7.081976\n",
      "1          2.967681            3.252070   9.582866\n",
      "2          5.472730            5.393467   1.448328\n",
      "3          2.538444            2.595351   2.241797\n",
      "4          3.892027            3.287219  15.539668\n",
      "5          3.334042            2.810117  15.714403\n",
      "6          4.511162            4.807198   6.562303\n",
      "7          4.313159            4.493429   4.179525\n",
      "8          2.057608            2.090026   1.575488\n",
      "9          3.798328            3.680514   3.101724\n",
      "10         2.974447            2.992158   0.595425\n",
      "11         2.621114            2.757521   5.204158\n",
      "12         2.729425            2.779050   1.818130\n",
      "13         4.223356            4.445476   5.259306\n",
      "14         4.147420            3.892288   6.151588\n",
      "15         4.004991            3.307339  17.419568\n",
      "16         3.436141            3.748229   9.082519\n",
      "17         2.009174            2.060931   2.576018\n",
      "18         3.524409            3.816070   8.275458\n",
      "19         2.881003            2.910200   1.013430\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "def logsigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip input to prevent overflow in exp()\n",
    "    return 1 / (1 + np.exp(-x)) # Log-sigmoid activation for hidden layer\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x  # Identity for output layer\n",
    "\n",
    "# Forward Pass: compute the network output given input X and weights\n",
    "def forward_pass(X, weights):\n",
    "    W1 = weights[:features * hidden_neuron].reshape(features, hidden_neuron)  # (5, hidden_neuron)\n",
    "    b1 = weights[features * hidden_neuron : features * hidden_neuron + hidden_neuron].reshape(1, hidden_neuron)  # (1, hidden_neuron)\n",
    "    W2 = weights[features * hidden_neuron + hidden_neuron : features * hidden_neuron + 2 * hidden_neuron].reshape(hidden_neuron, 1)  # (hidden_neuron, 1)\n",
    "    b2 = weights[-1]  # Single bias for output neuron\n",
    "    \n",
    "    hidden_layer = logsigmoid(np.dot(X, W1) + b1)  # Log-Sigmoid Activation\n",
    "    output_layer = identity(np.dot(hidden_layer, W2) + b2)  # Identity Activation\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "# Cost Function (LM requires residuals)\n",
    "def loss_function(weights, X, y):\n",
    "    y_pred = forward_pass(X, weights)\n",
    "    return (y_pred - y).ravel() # Flatten for least_squares\n",
    "\n",
    "\n",
    "# Standardize Input & Output\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y)  # Standardize y too!\n",
    "\n",
    "# Define Model Structure\n",
    "features = 4 # Number of input features\n",
    "hidden_neuron = 2  # Number of neurons in the hidden layer\n",
    "total_weights = (features * hidden_neuron) + hidden_neuron + hidden_neuron + 1  # Total number of trainable parameters\n",
    "\n",
    "# Split into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) \n",
    "\n",
    "# Initialize Weights Randomly\n",
    "initial_weights = np.random.randn(total_weights)  # 15 parameters\n",
    "\n",
    "# Apply Levenberg-Marquardt Optimization\n",
    "result = least_squares(loss_function, initial_weights, method='lm', args=(X_train, y_train))\n",
    "\n",
    "# Optimized Weights\n",
    "optimized_weights = result.x\n",
    "print(\"Optimized Weights:\\n\", optimized_weights)\n",
    "\n",
    "# Evaluate Model on Training and Test Data\n",
    "def evaluate_regression(X, y, weights, scaler_y):\n",
    "    y_pred = forward_pass(X, weights)\n",
    "    \n",
    "    # Inverse transform y_pred to original scale\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred)\n",
    "    y_original = scaler_y.inverse_transform(y)\n",
    "    \n",
    "    \n",
    "\n",
    "    mse = mean_squared_error(y_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_original, y_pred_original)\n",
    "    r2 = r2_score(y_original, y_pred_original)\n",
    "    \n",
    "    return mse, mae, r2, y_original, y_pred_original\n",
    "\n",
    "# Get Metrics for Training and Testing\n",
    "train_mse, train_mae, train_r2, y_train_original, y_train_pred_original = evaluate_regression(X_train, y_train, optimized_weights, scaler_y)\n",
    "test_mse, test_mae, test_r2, y_test_original, y_test_pred_original = evaluate_regression(X_test, y_test, optimized_weights, scaler_y)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "\n",
    "# Compare y_test and y_pred Side by Side (Original Scale)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual (y_test)': y_test_original.flatten(),\n",
    "    'Predicted (y_pred)': y_test_pred_original.flatten(),\n",
    "    'Error (%)': np.abs(((y_test_original.flatten()-y_test_pred_original.flatten())*100/y_test_original.flatten()))\n",
    "})\n",
    "\n",
    "# Print First 20 Rows\n",
    "print(comparison_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual (y)  Predicted (y_pred)  Error (%)\n",
      "0     4.858459            4.415081   9.125892\n",
      "1     3.496159            3.505521   0.267785\n",
      "2     3.165983            3.285643   3.779574\n",
      "3     2.966884            3.056347   3.015417\n",
      "4     4.360603            4.374529   0.319343\n",
      "5     3.132846            3.470539  10.779114\n",
      "6     2.967681            3.252070   9.582866\n",
      "7     2.734263            3.024281  10.606830\n",
      "8     2.321248            2.480031   6.840404\n",
      "9     2.057608            2.090026   1.575488\n",
      "10    4.331673            4.327774   0.090001\n",
      "11    3.190949            3.430141   7.495948\n",
      "12    3.021513            3.213281   6.346728\n",
      "13    2.763243            2.987213   8.105344\n",
      "14    2.268575            2.447350   7.880459\n",
      "15    2.009174            2.060931   2.576018\n",
      "16    4.383091            4.321899   1.396101\n",
      "17    3.124499            3.425060   9.619494\n",
      "18    3.057160            3.208400   4.947084\n",
      "19    2.766603            2.982548   7.805402\n",
      "20    2.294049            2.443232   6.503052\n",
      "21    2.031185            2.057263   1.283885\n",
      "22    5.187301            4.304230  17.023717\n",
      "23    3.721051            3.409772   8.365351\n",
      "24    3.437131            3.193715   7.081976\n",
      "25    3.125799            2.968508   5.032034\n",
      "26    2.575228            2.430838   5.606907\n",
      "27    2.245480            2.046216   8.874021\n",
      "28    4.546773            4.232913   6.902922\n",
      "29    3.210877            3.347963   4.269439\n",
      "30    2.995253            3.134315   4.642768\n",
      "31    2.742122            2.911689   6.183766\n",
      "32    2.269840            2.380604   4.879776\n",
      "33    2.043688            2.001382   2.070060\n",
      "34    4.436378            4.208908   5.127399\n",
      "35    3.169244            3.327122   4.981569\n",
      "36    2.965219            3.114277   5.026900\n",
      "37    2.720291            2.892510   6.330927\n",
      "38    2.261940            2.363622   4.495314\n",
      "39    2.008215            1.986205   1.096004\n",
      "40    2.712626            2.814567   3.758020\n",
      "41    2.247114            2.294472   2.107503\n",
      "42    2.015501            1.924296   4.525170\n",
      "43    4.423667            4.280876   3.227882\n",
      "44    3.491692            3.538446   1.338987\n",
      "45    2.974447            2.992158   0.595425\n",
      "46    2.557718            2.559360   0.064209\n",
      "47    5.598809            5.489474   1.952826\n",
      "48    4.434306            4.536194   2.297718\n",
      "49    3.677907            3.825395   4.010099\n",
      "Max Error (%): 18.41\n",
      "Min Error (%): 0.04\n",
      "Count of Errors > 10%: 31\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the Entire Dataset\n",
    "mse, mae, r2, y_original, y_pred_original = evaluate_regression(X, y, optimized_weights, scaler_y)\n",
    "# Create DataFrame to Compare\n",
    "full_comparison_df = pd.DataFrame({\n",
    "    'Actual (y)': y_original.flatten(),\n",
    "    'Predicted (y_pred)': y_pred_original.flatten(),\n",
    "    'Error (%)': np.abs(((y_original.flatten() - y_pred_original.flatten()) * 100 / y_original.flatten()))\n",
    "})\n",
    "\n",
    "# Display Results\n",
    "print(full_comparison_df.head(50))  # Show first 50 rows\n",
    "\n",
    "# Error Statistics\n",
    "print(f\"Max Error (%): {full_comparison_df['Error (%)'].max():.2f}\")\n",
    "print(f\"Min Error (%): {full_comparison_df['Error (%)'].min():.2f}\")\n",
    "error_above_10_percent = (full_comparison_df['Error (%)'] > 10).sum()\n",
    "print(f\"Count of Errors > 10%: {error_above_10_percent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Means: [7.01946721 2.06454918 0.53210656 0.06766475]\n",
      "X Standard Deviations: [3.82262397 0.71274148 0.21078035 0.03034432]\n",
      "y Mean: 3.6378581098196467\n",
      "y Standard Deviation: 1.2040775968079265\n"
     ]
    }
   ],
   "source": [
    "# Get mean and standard deviation of input features\n",
    "X_means = scaler_X.mean_\n",
    "X_stds = scaler_X.scale_\n",
    "\n",
    "# Get mean and standard deviation of target variable\n",
    "y_mean = scaler_y.mean_[0]\n",
    "y_std = scaler_y.scale_[0]\n",
    "\n",
    "print(\"X Means:\", X_means)\n",
    "print(\"X Standard Deviations:\", X_stds)\n",
    "print(\"y Mean:\", y_mean)\n",
    "print(\"y Standard Deviation:\", y_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse scale all data\n",
    "X_all = np.vstack((X_train, X_test))  # Combine training and test data\n",
    "X_all_original = scaler_X.inverse_transform(X_all)  # Reverse scale input features\n",
    "y_all_actual = np.vstack((y_train_original, y_test_original))  # Actual values\n",
    "y_all_predicted = np.vstack((y_train_pred_original, y_test_pred_original))  # Predicted values\n",
    "\n",
    "# Create DataFrame with all data\n",
    "final_df = pd.DataFrame(X_all_original)  # Restore original feature names\n",
    "final_df['Actual Kd'] = y_all_actual.flatten()  # Add actual y values\n",
    "final_df['Predicted Kd'] = y_all_predicted.flatten()  # Add predicted y values\n",
    "\n",
    "# Save to Excel\n",
    "#final_df.to_excel('All_data_results2Neuron.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
