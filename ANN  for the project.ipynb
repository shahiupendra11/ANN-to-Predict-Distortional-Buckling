{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Dataset\n",
    "Here we will import all the necessary libraries and load the dataset as df and separate the features ('H', 'B', 'D', 't') as X and target ('Kd') as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       H     B      D       t        Kd\n",
      "0  1.625  1.25  0.188  0.0188  4.858459\n",
      "1  1.625  1.25  0.188  0.0283  3.496159\n",
      "2  1.625  1.25  0.188  0.0312  3.165983\n",
      "3  1.625  1.25  0.188  0.0346  2.966884\n",
      "4  2.500  1.25  0.188  0.0188  4.360603\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import least_squares\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load Dataset (Assuming 'df' is already loaded)\n",
    "df = pd.read_excel('test.xlsx')\n",
    "# 'Kd' is the target, and all other columns are features\n",
    "X = df[['H', 'B', 'D', 't']].values  # 4 input features\n",
    "y = df['Kd'].values.reshape(-1, 1)  # Ensure correct shape\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network\n",
    "This notebook implements a simple feedforward neural network with one hidden layer using a log-sigmoid activation for the hidden neurons and identity activation for the output. The model is trained to perform regression using Levenberg-Marquardt optimization (scipy.optimize.least_squares).\n",
    "Key steps:\n",
    "\n",
    "Standardization of input and output features using StandardScaler for better convergence.\n",
    "\n",
    "Manual forward pass implementation with custom activation functions.\n",
    "\n",
    "Loss function computes residuals for the optimizer.\n",
    "\n",
    "Model structure: 4 input features, 2 hidden neurons, and 1 output neuron.\n",
    "\n",
    "Training/test split, weight initialization, and optimization.\n",
    "\n",
    "Model evaluation using MSE, MAE, and R¬≤ on both training and test sets.\n",
    "\n",
    "Prediction comparison in original scale for interpretability.\n",
    "\n",
    "The final output includes model performance metrics and a table comparing actual vs predicted values with percentage error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Weights:\n",
      " [-0.0437315   0.01131759  0.5960148  -0.64389609  0.58718547 -1.74043342\n",
      " -2.65663108  1.40109385 -3.2485941  -0.67903587  4.82714864 -1.46562389\n",
      "  0.00827682]\n",
      "Training MSE: 0.1018, MAE: 0.2022, R¬≤: 0.9282\n",
      "Test MSE: 0.0447, MAE: 0.1818, R¬≤: 0.9702\n",
      "    Actual (y_test)  Predicted (y_pred)  Error (%)\n",
      "0          3.437131            3.217400   6.392878\n",
      "1          2.967681            3.232247   8.914885\n",
      "2          5.472730            5.111147   6.606998\n",
      "3          2.538444            2.438436   3.939751\n",
      "4          3.892027            3.755649   3.504017\n",
      "5          3.334042            3.599685   7.967589\n",
      "6          4.511162            4.817465   6.789906\n",
      "7          4.313159            4.078242   5.446527\n",
      "8          2.057608            2.152428   4.608259\n",
      "9          3.798328            3.671162   3.347942\n",
      "10         2.974447            2.680606   9.878852\n",
      "11         2.621114            2.685669   2.462867\n",
      "12         2.729425            2.700868   1.046297\n",
      "13         4.223356            4.040486   4.329972\n",
      "14         4.147420            3.907182   5.792467\n",
      "15         4.004991            3.761988   6.067511\n",
      "16         3.436141            3.322061   3.320002\n",
      "17         2.009174            2.150695   7.043741\n",
      "18         3.524409            3.363850   4.555646\n",
      "19         2.881003            3.103768   7.732218\n"
     ]
    }
   ],
   "source": [
    "# Activation Functions\n",
    "def logsigmoid(x):\n",
    "    x = np.clip(x, -500, 500)  # Clip input to prevent overflow in exp()\n",
    "    return 1 / (1 + np.exp(-x)) # Log-sigmoid activation for hidden layer\n",
    "\n",
    "\n",
    "def identity(x):\n",
    "    return x  # Identity for output layer\n",
    "\n",
    "# Forward Pass: compute the network output given input X and weights\n",
    "def forward_pass(X, weights):\n",
    "    W1 = weights[:features * hidden_neuron].reshape(features, hidden_neuron)  # (5, hidden_neuron)\n",
    "    b1 = weights[features * hidden_neuron : features * hidden_neuron + hidden_neuron].reshape(1, hidden_neuron)  # (1, hidden_neuron)\n",
    "    W2 = weights[features * hidden_neuron + hidden_neuron : features * hidden_neuron + 2 * hidden_neuron].reshape(hidden_neuron, 1)  # (hidden_neuron, 1)\n",
    "    b2 = weights[-1]  # Single bias for output neuron\n",
    "    \n",
    "    hidden_layer = logsigmoid(np.dot(X, W1) + b1)  # Log-Sigmoid Activation\n",
    "    output_layer = identity(np.dot(hidden_layer, W2) + b2)  # Identity Activation\n",
    "\n",
    "    return output_layer\n",
    "\n",
    "# Cost Function (LM requires residuals)\n",
    "def loss_function(weights, X, y):\n",
    "    y_pred = forward_pass(X, weights)\n",
    "    return (y_pred - y).ravel() # Flatten for least_squares\n",
    "\n",
    "\n",
    "# Standardize Input & Output\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X = scaler_X.fit_transform(X)\n",
    "y = scaler_y.fit_transform(y)  # Standardize y too!\n",
    "\n",
    "# Define Model Structure\n",
    "features = 4 # Number of input features\n",
    "hidden_neuron = 2  # Number of neurons in the hidden layer\n",
    "total_weights = (features * hidden_neuron) + hidden_neuron + hidden_neuron + 1  # Total number of trainable parameters\n",
    "\n",
    "# Split into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42) \n",
    "\n",
    "# Initialize Weights Randomly\n",
    "initial_weights = np.random.randn(total_weights)  # 15 parameters\n",
    "\n",
    "# Apply Levenberg-Marquardt Optimization\n",
    "result = least_squares(loss_function, initial_weights, method='lm', args=(X_train, y_train))\n",
    "\n",
    "# Optimized Weights\n",
    "optimized_weights = result.x\n",
    "print(\"Optimized Weights:\\n\", optimized_weights)\n",
    "\n",
    "# Evaluate Model on Training and Test Data\n",
    "def evaluate_regression(X, y, weights, scaler_y):\n",
    "    y_pred = forward_pass(X, weights)\n",
    "    \n",
    "    # Inverse transform y_pred to original scale\n",
    "    y_pred_original = scaler_y.inverse_transform(y_pred)\n",
    "    y_original = scaler_y.inverse_transform(y)\n",
    "    \n",
    "    \n",
    "\n",
    "    mse = mean_squared_error(y_original, y_pred_original)\n",
    "    mae = mean_absolute_error(y_original, y_pred_original)\n",
    "    r2 = r2_score(y_original, y_pred_original)\n",
    "    \n",
    "    return mse, mae, r2, y_original, y_pred_original\n",
    "\n",
    "# Get Metrics for Training and Testing\n",
    "train_mse, train_mae, train_r2, y_train_original, y_train_pred_original = evaluate_regression(X_train, y_train, optimized_weights, scaler_y)\n",
    "test_mse, test_mae, test_r2, y_test_original, y_test_pred_original = evaluate_regression(X_test, y_test, optimized_weights, scaler_y)\n",
    "\n",
    "# Print Results\n",
    "print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R¬≤: {train_r2:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R¬≤: {test_r2:.4f}\")\n",
    "\n",
    "# Compare y_test and y_pred Side by Side (Original Scale)\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual (y_test)': y_test_original.flatten(),\n",
    "    'Predicted (y_pred)': y_test_pred_original.flatten(),\n",
    "    'Error (%)': np.abs(((y_test_original.flatten()-y_test_pred_original.flatten())*100/y_test_original.flatten()))\n",
    "})\n",
    "\n",
    "# Print First 20 Rows\n",
    "print(comparison_df.head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation on Entire Dataset\n",
    "This section evaluates the performance of the trained neural network on the entire dataset (training + testing combined) to give a holistic view of its predictive accuracy.\n",
    "\n",
    "Key Steps:\n",
    "Prediction: The model performs a forward pass through the entire input data to generate predicted outputs in standardized form, which are then inverse-transformed to the original scale.\n",
    "\n",
    "Comparison DataFrame: A table is created comparing actual vs predicted values along with the percentage error for each data point.\n",
    "\n",
    "$$\n",
    "\\text{Error (\\%)} = \\left| \\frac{\\text{Actual} - \\text{Predicted}}{\\text{Actual}} \\right| \\times 100\n",
    "$$\n",
    "\n",
    "Statistics Printed:\n",
    "Max Error (%): The largest prediction error across the dataset.\n",
    "Min Error (%): The smallest error observed.\n",
    "Count of Errors > 10%: Number of data points where the prediction error exceeds 10%, helping identify outlier predictions or areas needing improvement.\n",
    "\n",
    "This analysis helps assess model robustness and generalization by identifying the spread and distribution of prediction errors across all samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Actual (y)  Predicted (y_pred)  Error (%)\n",
      "0     4.858459            4.626446   4.775448\n",
      "1     3.496159            3.503703   0.215768\n",
      "2     3.165983            3.240981   2.368889\n",
      "3     2.966884            2.980358   0.454162\n",
      "4     4.360603            4.611923   5.763407\n",
      "5     3.132846            3.493517  11.512551\n",
      "6     2.967681            3.232247   8.914885\n",
      "7     2.734263            2.973207   8.738871\n",
      "8     2.321248            2.438856   5.066580\n",
      "9     2.057608            2.152428   4.608259\n",
      "10    4.331673            4.595371   6.087675\n",
      "11    3.190949            3.481946   9.119424\n",
      "12    3.021513            3.222331   6.646259\n",
      "13    2.763243            2.965093   7.304837\n",
      "14    2.268575            2.434854   7.329662\n",
      "15    2.009174            2.150695   7.043741\n",
      "16    4.383091            4.593305   4.796028\n",
      "17    3.124499            3.480504  11.394011\n",
      "18    3.057160            3.221096   5.362391\n",
      "19    2.766603            2.964084   7.138013\n",
      "20    2.294049            2.434357   6.116147\n",
      "21    2.031185            2.150480   5.873182\n",
      "22    5.187301            4.587113  11.570333\n",
      "23    3.721051            3.476188   6.580472\n",
      "24    3.437131            3.217400   6.392878\n",
      "25    3.125799            2.961061   5.270277\n",
      "26    2.575228            2.432867   5.528120\n",
      "27    2.245480            2.149835   4.259455\n",
      "28    4.546773            4.562417   0.344064\n",
      "29    3.210877            3.459026   7.728402\n",
      "30    2.995253            3.202713   6.926291\n",
      "31    2.742122            2.949058   7.546555\n",
      "32    2.269840            2.426957   6.921933\n",
      "33    2.043688            2.147276   5.068683\n",
      "34    4.436378            4.554210   2.656041\n",
      "35    3.169244            3.453343   8.964227\n",
      "36    2.965219            3.197852   7.845410\n",
      "37    2.720291            2.945088   8.263745\n",
      "38    2.261940            2.425005   7.209071\n",
      "39    2.008215            2.146431   6.882515\n",
      "40    2.712626            2.929368   7.990112\n",
      "41    2.247114            2.417283   7.572802\n",
      "42    2.015501            2.143087   6.330234\n",
      "43    4.423667            4.125216   6.746678\n",
      "44    3.491692            3.245125   7.061540\n",
      "45    2.974447            2.680606   9.878852\n",
      "46    2.557718            2.295444  10.254223\n",
      "47    5.598809            5.193806   7.233728\n",
      "48    4.434306            4.112155   7.264969\n",
      "49    3.677907            3.369664   8.380938\n",
      "Max Error (%): 67.19\n",
      "Min Error (%): 0.22\n",
      "Count of Errors > 10%: 14\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the Entire Dataset\n",
    "mse, mae, r2, y_original, y_pred_original = evaluate_regression(X, y, optimized_weights, scaler_y)\n",
    "# Create DataFrame to Compare\n",
    "full_comparison_df = pd.DataFrame({\n",
    "    'Actual (y)': y_original.flatten(),\n",
    "    'Predicted (y_pred)': y_pred_original.flatten(),\n",
    "    'Error (%)': np.abs(((y_original.flatten() - y_pred_original.flatten()) * 100 / y_original.flatten()))\n",
    "})\n",
    "\n",
    "# Display Results\n",
    "print(full_comparison_df.head(50))  # Show first 50 rows\n",
    "\n",
    "# Error Statistics\n",
    "print(f\"Max Error (%): {full_comparison_df['Error (%)'].max():.2f}\")\n",
    "print(f\"Min Error (%): {full_comparison_df['Error (%)'].min():.2f}\")\n",
    "error_above_10_percent = (full_comparison_df['Error (%)'] > 10).sum()\n",
    "print(f\"Count of Errors > 10%: {error_above_10_percent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting and Inspecting Neural Network Weights and Biases\n",
    "This section retrieves and displays the learned parameters of the trained neural network after optimization using the Levenberg‚ÄìMarquardt algorithm.\n",
    "\n",
    "üîç Step-by-Step Explanation:\n",
    "1. Mean and Standard Deviation of Features:\n",
    "These are extracted from StandardScaler after standardizing the input features and target variable.\n",
    "\n",
    "Used for inverse transformation and interpretation of results in original units. \n",
    " \n",
    "2. Weight Extraction Function:\n",
    "The function extract_weights() decomposes the flattened 1D optimized_weights vector into separate matrices and vectors:\n",
    "\n",
    "Input-to-Hidden Weights ($W_1$):\n",
    "\n",
    "Hidden Layer Biases ($b_1$):\n",
    "\n",
    "Hidden-to-Output Weights ($W_2$):\n",
    "\n",
    "Output Layer Bias ($b_2$):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Means: [7.01946721 2.06454918 0.53210656 0.06766475]\n",
      "X Standard Deviations: [3.82262397 0.71274148 0.21078035 0.03034432]\n",
      "y Mean: 3.6378581098196467\n",
      "y Standard Deviation: 1.2040775968079265\n",
      "Input-to-Hidden Weights (W1):\n",
      " [[-0.0437315   0.01131759]\n",
      " [ 0.5960148  -0.64389609]\n",
      " [ 0.58718547 -1.74043342]\n",
      " [-2.65663108  1.40109385]]\n",
      "Hidden Biases (b1):\n",
      " [[-3.2485941  -0.67903587]]\n",
      "Hidden-to-Output Weights (W2):\n",
      " [[ 4.82714864]\n",
      " [-1.46562389]]\n",
      "Output Bias (b2):\n",
      " 0.008276820143825815\n"
     ]
    }
   ],
   "source": [
    "# Get mean and standard deviation of input features\n",
    "X_means = scaler_X.mean_\n",
    "X_stds = scaler_X.scale_\n",
    "# Get mean and standard deviation of target variable\n",
    "y_mean = scaler_y.mean_[0]\n",
    "y_std = scaler_y.scale_[0]\n",
    "print(\"X Means:\", X_means)\n",
    "print(\"X Standard Deviations:\", X_stds)\n",
    "print(\"y Mean:\", y_mean)\n",
    "print(\"y Standard Deviation:\", y_std)\n",
    "\n",
    "def extract_weights(optimized_weights, features, hidden_neuron):\n",
    "    # Input-to-hidden weights (W1): shape (features, hidden_neuron)\n",
    "    W1 = optimized_weights[:features * hidden_neuron].reshape(features, hidden_neuron)\n",
    "    # Hidden layer biases (b1): shape (1, hidden_neuron)\n",
    "    b1_start = features * hidden_neuron\n",
    "    b1_end = b1_start + hidden_neuron\n",
    "    b1 = optimized_weights[b1_start:b1_end].reshape(1, hidden_neuron)\n",
    "    # Hidden-to-output weights (W2): shape (hidden_neuron, 1)\n",
    "    W2_start = b1_end\n",
    "    W2_end = W2_start + hidden_neuron\n",
    "    W2 = optimized_weights[W2_start:W2_end].reshape(hidden_neuron, 1)\n",
    "    # Output bias (b2): scalar\n",
    "    b2 = optimized_weights[-1]\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Extract the weight matrices and bias vectors\n",
    "W1, b1, W2, b2 = extract_weights(optimized_weights, features, hidden_neuron)\n",
    "\n",
    "# Print in human-readable form\n",
    "print(\"Input-to-Hidden Weights (W1):\\n\", W1)\n",
    "print(\"Hidden Biases (b1):\\n\", b1)\n",
    "print(\"Hidden-to-Output Weights (W2):\\n\", W2)\n",
    "print(\"Output Bias (b2):\\n\", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results: Combine, Inverse Transform, and Export\n",
    "This section prepares a complete dataset containing both the original input features and the predicted vs actual target values (in their original scale), useful for final analysis, visualization, or reporting.\n",
    "\n",
    "Step-by-Step Explanation:\n",
    "1. Combine Datasets:\n",
    "\n",
    "The training and testing sets (X_train, X_test) are vertically stacked using np.vstack() to form a unified input dataset X_all.\n",
    "\n",
    "Similarly, corresponding predicted and actual outputs are combined.\n",
    "\n",
    "2. Inverse Transformation of Input Features:\n",
    "\n",
    "The standardized inputs (X_all) are converted back to their original scale using scaler_X.inverse_transform(), giving you interpretable feature values.\n",
    "\n",
    "3. Construct Final DataFrame:\n",
    "\n",
    "A pandas DataFrame is created containing:\n",
    "\n",
    "Original input features (unstandardized),\n",
    "\n",
    "Actual output values (Actual Kd),\n",
    "\n",
    "Predicted output values (Predicted Kd).\n",
    "\n",
    "4. Save to Excel:\n",
    "\n",
    "The final DataFrame can be exported to an Excel file (e.g., All_data_results2Neuron.xlsx) using to_excel(). This line is commented out but can be activated for saving.\n",
    "\n",
    "This step is ideal for final reporting, error analysis, or sharing results with others in a format that‚Äôs easy to understand and work with (e.g., in Excel or CSV)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse scale all data\n",
    "X_all = np.vstack((X_train, X_test))  # Combine training and test data\n",
    "X_all_original = scaler_X.inverse_transform(X_all)  # Reverse scale input features\n",
    "y_all_actual = np.vstack((y_train_original, y_test_original))  # Actual values\n",
    "y_all_predicted = np.vstack((y_train_pred_original, y_test_pred_original))  # Predicted values\n",
    "\n",
    "# Create DataFrame with all data\n",
    "final_df = pd.DataFrame(X_all_original)  # Restore original feature names\n",
    "final_df['Actual Kd'] = y_all_actual.flatten()  # Add actual y values\n",
    "final_df['Predicted Kd'] = y_all_predicted.flatten()  # Add predicted y values\n",
    "\n",
    "# Save to Excel\n",
    "#final_df.to_excel('All_data_results2Neuron.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
